**Workshop 01: Recruitment Data Warehouse**
By: David Santa

**1. Project objective**

The project's goal is to design, populate and validate a small-scale data warehouse (DW) to support analysis of a recruitment pipeline. 

From a source CSV of applications we:

-Define a dimensional model (star schema)

-Implement an ETL process (Python, pandas, SQLAlchemy) that normalizes and validates data

-Persist the schema and data to a relational DB (MySQL)

-Expose SQL views that compute business KPIs ready for visualization.

Deliverables include the DW DDL, the ETL code, Power BI file and a set of SQL views containing the KPIs used by the Power BI report.

**2. Star schema and design decisions**

Dimensions

dim_date: date_key is an integer YYYYMMDD surrogate (derived in ETL). Also stores full_date (DATE), year, month, day.

dim_candidate: candidate dimension with candidate_key (surrogate) and attributes first_name, last_name, email. candidate_key and email are used as a composed PK.

dim_country, dim_technology, dim_seniority: simple lookup dimensions with surrogate keys and a single business attribute (country, technology, seniority).

Fact Table

fact_application: the fact table at the application grain. Key fields:

application_key (surrogate PK),

metrics: code_challenge_score, technical_interview_score, yoe, hired_flag (0/1),

FK's to dimensions: candidate_key, country_key, technology_key, seniority_key, date_key,

Key choices

Use date_key as derived YYYYMMDD (calculated in ETL).

Other dimensions use surrogate keys generated by the ETL process rather than an autoincrement surrogate keys in MySQL.

I also implemented an ETL-side anti-join to prevent duplicate fact inserts and INSERT IGNORE pattern for dimensions.

**3. Grain definition**

Fact grain: one row per candidate application to a specific technology / seniority / country on a given application date.

**4. ETL logic**

Using as source a CSV with columns: First Name, Last Name, Email, Application Date, Country, YOE, Seniority, Technology, Code Challenge Score, Technical Interview Score. The ETL process consist of:

1. Standardize and rename incoming columns to DW field names (e.g., Code Challenge Score to code_challenge_score). Strip whitespace and lower cases.

2. Build dim_date: Convert Application Date to date_dim, derive date_key = YYYYMMDD and populate year, month, day and full-date (YYYY-MM-DD).

3. Build business dimensions: Extract unique values for country, technology, seniority

4. Construct fact_application:

Calculate hired_flag using the explicit business rule

hired_flag = 1 if code_challenge_score >= 7 AND technical_interview_score >= 7, otherwise 0.

Dimensions load via temporary tables + INSERT IGNORE to avoid duplicates and permit repeated runs by multiple users.

ETL performs an anti-join against existing logical keys (candidate_key, technology_key, seniority_key, country_key, date_key) and inserts only new rows.

**5. Data quality assumptions**

Score domains: code_challenge_score and technical_interview_score are numeric in the expected 0–10 range. Out-of-range or non-numeric values are and excluded from aggregates.

Candidate identity: candidate_key and email are treated as a composed PK.

Multiple executions must not create duplicate analytical rows. This is achieved via ETL anti-join logic.

**6. How to run the project**

Prerequisites:

**Warning This project is supposed to be runned on Windows, Mac/Linux alternatives are unknown**

Python 3.10+ and a virtual environment (optional).

A MySQL server reachable by the ETL.

Required Python packages: listed in requirements.txt (install via pip install -r requirements.txt).

MySQL Workbench for schema and view validation.

MySQL Connector/ODBC (version 8.x/64bit).

Power BI Desktop for visualization (64bit).

Run steps:

1. Create and activate a virtual environment (optional)

        python -m venv .etl
        .etl\Scripts\activate

2. Install dependencies:

        pip install -r requirements.txt
   
3. Ensure the target database recruitment_dw exists and run the provided DDL file (recruitment_dw.sql) in MySQL Workbench if needed.

4. Execute the ETL pipeline

5. Verify counts in MySQL Workbench. Example:

        SELECT COUNT(*) FROM recruitment_dw.dim_date;
        SELECT COUNT(*) FROM recruitment_dw.dim_candidate;
        SELECT COUNT(*) FROM recruitment_dw.fact_application;

5.1 Example Outputs:

Fact table: fact_application

<img width="1149" height="439" alt="image" src="https://github.com/user-attachments/assets/4d1fefe0-8b41-4615-9f50-fbc49486271c" />

dimension: dim_candidate

<img width="787" height="439" alt="image" src="https://github.com/user-attachments/assets/ff69eb0e-72de-4274-bb9f-0e1e4a449fdc" />

dimension: dim_date

<img width="555" height="443" alt="image" src="https://github.com/user-attachments/assets/2216ecef-5673-48b1-8a48-89b9a347f99b" />

Some examples for the views in the recruitment_dw.sql file:

        SELECT * FROM recruitment_dw.hires_by_year_country_vw;

<img width="720" height="440" alt="image" src="https://github.com/user-attachments/assets/0f4f1eaf-3cfa-4710-ba30-3e7e0c375171" />

        SELECT * FROM recruitment_dw.hire_rate;

<img width="733" height="305" alt="image" src="https://github.com/user-attachments/assets/bc1fd1a7-3ef7-4e00-a93b-e6f55f63e66d" />

        SELECT * FROM recruitment_dw.hires_by_technology_vw;

<img width="734" height="434" alt="image" src="https://github.com/user-attachments/assets/e4a6c16d-3b28-4dcd-a9c7-2abbdf2c3d6f" />

These are 3 of 6 total views included in the recruitment_dw.sql file

6. Connecting MySQL Workbench to Power BI (ODBC)
        
This project consumes the Data Warehouse from Power BI through an ODBC Data Source Name (DSN) configured with the MySQL ODBC driver.

7. Creating the ODBC DNS

1. Open ODBC Data Sources (64-bit) in Windows.

2. Go to the System DSN tab (recommended for multi-user machines).

3. Click Add…

4. Select MySQL ODBC 8.0 Unicode Driver.

5. Configure the DSN with the following fields:

   Data Source Name: recruitment_dw_dns

  TCP/IP Server: localhost

  Port: 3306

  User: <your_MySQL_Workbench_user>

  Password: <your_password>

  Database: recruitment_dw

7. Click Test to verify connectivity.

8. Click OK to save the DNS.

9. Using the Included Power BI Report (.pbix)

This repository already contains a prebuilt Power BI report. Follow the steps below to connect it to your local Data Warehouse after cloning the project.

9.1 Locate the file

workshop_01/diagrams/power_bi_kpis/recruitment_kpi_visualization.pbix

9.2 Open it with Power BI and update Data Source Credentials

1. In Power BI go to: Home → Transform Data → Data Source Settings

2. Select the ODBC source.

3. Click Edit Permissions.

4. Enter your MySQL credentials if prompted.

5. Confirm that the DSN used is: recruitment_dw_dns

6.Refresh the dataset: Home → Refresh

  After refreshing, the report should display KPIs including:
  
  Hires by Technology
  
  Hires by Year
  
  Hires by Seniority
  
  Hires by Country over Years (One of the requirements to this KPI is that it should be focused on USA, Brazil, Colombia and Ecuador. You may use the right sidebar, click on the line graph of the dashboard, then use the Filter section and on the search bar of "country" write and select the countries you want to analize, you may select more than one if you want to see the behavior of this KPI over the years)
  
  Hire Rate (%)

  Average Scores

You should be able to the Power Bi report and the values should match the contents of the recruitment_dw database. Here is an image of how it should look:

<img width="1432" height="810" alt="image" src="https://github.com/user-attachments/assets/4480692c-29b4-4bbf-987a-61f9cf2ce222" />

