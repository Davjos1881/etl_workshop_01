**Workshop 01: Recruitment Data Warehouse**
By: David Santa

**1. Project objective**

The project's goal is to design, populate and validate a small-scale data warehouse (DW) to support analysis of a recruitment pipeline. 

From a source CSV of applications we:

-Define a dimensional model (star schema)

-Implement an ETL process (Python, pandas, SQLAlchemy) that normalizes and validates data

-Persist the schema and data to a relational DB (MySQL)

-Expose SQL views that compute business KPIs ready for visualization.

Deliverables include the DW DDL, the ETL code, Power BI file and a set of SQL views containing the KPIs used by the Power BI report.

**2. Star schema and design decisions**

Dimensions

dim_date: date_key is an integer YYYYMMDD surrogate (derived in ETL). Also stores full_date (DATE), year, month, day.

dim_candidate: candidate dimension with candidate_key (surrogate) and attributes first_name, last_name, email. candidate_key and email are used as a composed PK.

dim_country, dim_technology, dim_seniority: simple lookup dimensions with surrogate keys and a single business attribute (country, technology, seniority).

Fact Table

fact_application: the fact table at the application grain. Key fields:

application_key (surrogate PK),

metrics: code_challenge_score, technical_interview_score, yoe, hired_flag (0/1),

FK's to dimensions: candidate_key, country_key, technology_key, seniority_key, date_key,

Key choices

Use date_key as derived YYYYMMDD (calculated in ETL) rather than an auto-increment surrogate key generated by MySQL.

Other dimensions use surrogate keys generates automatically by MySQL Workbench.

I also implemented an ETL-side anti-join to prevent duplicate fact inserts and INSERT IGNORE pattern for dimensions.

**3. Grain definition**

Fact grain: one row per candidate application to a specific technology / seniority / country on a given application date.

**4. ETL logic**

Using as source a CSV with columns: First Name, Last Name, Email, Application Date, Country, YOE, Seniority, Technology, Code Challenge Score, Technical Interview Score. The ETL process consist of:

1. Standardize and rename incoming columns to DW field names (e.g., Code Challenge Score to code_challenge_score). Strip whitespace and lower cases.

2. Build dim_date: Convert Application Date to date_dim, derive date_key = YYYYMMDD and populate year, month, day and full-date (YYYY-MM-DD).

3. Build business dimensions: Extract unique values for country, technology, seniority

4. Construct fact_application:

Calculate hired_flag using the explicit business rule

hired_flag = 1 if code_challenge_score >= 7 AND technical_interview_score >= 7, otherwise 0.

Dimensions load via temporary tables + INSERT IGNORE to avoid duplicates and permit repeated runs by multiple users.

ETL performs an anti-join against existing logical keys (candidate_key, technology_key, seniority_key, country_key, date_key) and inserts only new rows.

**5. Data quality assumptions**

Score domains: code_challenge_score and technical_interview_score are numeric in the expected 0â€“10 range. Out-of-range or non-numeric values are and excluded from aggregates.

Candidate identity: candidate_key and email are treated as a composed PK.

Multiple executions must not create duplicate analytical rows. This is achieved via ETL anti-join logic.

**6. How to run the project**

Prerequisites

Python 3.10+ and a virtual environment (optional).

A MySQL server reachable by the ETL.

Required Python packages: listed in requirements.txt (install via pip install -r requirements.txt).

MySQL Workbench for schema and view validation.

MySQL Connector/ODBC (version 8.x/64bit)

Power BI Desktop for visualization (64bit).
